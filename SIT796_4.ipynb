{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "SIT796-4.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i29Gd1JoXe4f",
        "outputId": "90388c4e-9d0d-4c59-aed1-e90a5f710bdd"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from matplotlib import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from collections import defaultdict\n",
        "from functools import partial\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "environment = gym.make('Blackjack-v0')\n",
        "#Define a policy where we hit until we reach 19.\n",
        "# actions here are 0-stand, 1-hit\n",
        "def policy_sample(observation):\n",
        "  score, dealer_score, usable_ace = observation\n",
        "  return 0 if score >= 19 else 1\n",
        "\n",
        "def generate_episode(policy, environment): \n",
        "    states, actions, rewards = [], [], []\n",
        "    observation = environment.reset()  \n",
        "    while True:\n",
        "        states.append(observation)\n",
        "        action = policy_sample(observation)\n",
        "        actions.append(action)\n",
        "        observation, reward, done, info = environment.step(action)\n",
        "        rewards.append(reward)\n",
        "        if done:\n",
        "            break\n",
        "    return states, actions, rewards\n",
        "def first_visit_mc_prediction(policy, environment, n_episodes):\n",
        "    value_table = defaultdict(float)\n",
        "    N = defaultdict(int)\n",
        "    for _ in range(n_episodes):\n",
        "        states, _, rewards = generate_episode(policy, environment)\n",
        "        returns = 0\n",
        "        for t in range(len(states) -1, -1, -1):\n",
        "            R = rewards[t]\n",
        "            S = states[t]\n",
        "            returns += R\n",
        "        if S not in states[:t]:\n",
        "            N[S] += 1\n",
        "            value_table[S] += (returns - value_table[S]) / N[S]\n",
        "    return value_table\n",
        "\n",
        "value = first_visit_mc_prediction(policy_sample, environment, n_episodes=500000)\n",
        "for i in range(10):\n",
        "  print(value.popitem())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "((12, 9, True), -0.17622950819672137)\n",
            "((12, 8, True), 0.050228310502283116)\n",
            "((4, 5, False), -0.25789473684210523)\n",
            "((18, 5, True), -0.21889400921659)\n",
            "((12, 1, True), -0.2465753424657533)\n",
            "((17, 8, True), -0.1719745222929935)\n",
            "((20, 6, True), 0.6469387755102042)\n",
            "((4, 7, False), -0.15454545454545457)\n",
            "((19, 8, True), 0.5514018691588785)\n",
            "((17, 7, True), -0.2416107382550337)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqvnDNk3Xe4n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuEmHLtoXe4o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}